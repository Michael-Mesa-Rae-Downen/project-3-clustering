{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e3cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing of all needed libraries and modules.  \n",
    "%matplotlib inline\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import SelectKBest, RFE, f_regression, SequentialFeatureSelector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LassoLars, LinearRegression, TweedieRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import env\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import wrangle as w\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62af5aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=w.wrangle_zillow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb4dcf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = w.split_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea4fd06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled, validate_scaled, test_scaled=w.scale_data(train, validate, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd18e557",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = train_scaled[['bedrooms','bathrooms', 'sq_feet']]\n",
    "y_train=train[['logerror']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46517eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validate_scaled=validate_scaled[['bedrooms','bathrooms', 'sq_feet']]\n",
    "y_validate=validate[['logerror']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f12a7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled=test_scaled[['bedrooms','bathrooms', 'sq_feet']]\n",
    "y_test=test[['logerror']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e10fa0",
   "metadata": {},
   "source": [
    "BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fc6eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a baseline prediction by creating a function.\n",
    "def baseline(X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test):\n",
    "    # We need y_train and y_validate (and test) to be dataframes to append the new columns with predicted values. \n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_validate = pd.DataFrame(y_validate)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "\n",
    "    # 1. Predict home_value_pred_mean\n",
    "    logerror_pred_mean = y_train['logerror'].mean()\n",
    "    y_train['logerror_pred_mean'] = logerror_pred_mean\n",
    "    y_validate['logerror_pred_mean'] = logerror_pred_mean\n",
    "\n",
    "    # 2. compute home_value_pred_median\n",
    "    logerror_pred_median = y_train['logerror'].median()\n",
    "    y_train['logerror_pred_median'] = logerror_pred_median\n",
    "    y_validate['logerror_pred_median'] = logerror_pred_median\n",
    "\n",
    "    # 3. RMSE of home_value_pred_mean\n",
    "    rmse_train = mean_squared_error(y_train[['logerror']], y_train.logerror_pred_mean)**(1/2)\n",
    "    rmse_validate = mean_squared_error(y_validate[['logerror']], y_validate.logerror_pred_mean)**(1/2)\n",
    "\n",
    "    print(\"RMSE using Mean\\nTrain/In-Sample: \", (rmse_train), \n",
    "      \"\\nValidate/Out-of-Sample: \", (rmse_validate))\n",
    "\n",
    "    # 4. RMSE of home_value_pred_median\n",
    "    rmse_train = mean_squared_error(y_train[['logerror']], y_train.logerror_pred_median)**(1/2)\n",
    "    rmse_validate = mean_squared_error(y_validate[['logerror']], y_validate.logerror_pred_median)**(1/2)\n",
    "\n",
    "    print(\"RMSE using Median\\nTrain/In-Sample: \", (rmse_train), \n",
    "      \"\\nValidate/Out-of-Sample: \", (rmse_validate))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1b122a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE using Mean\n",
      "Train/In-Sample:  0.17111493570816058 \n",
      "Validate/Out-of-Sample:  0.17619004134685448\n",
      "RMSE using Median\n",
      "Train/In-Sample:  0.17149111174671647 \n",
      "Validate/Out-of-Sample:  0.17662277642319174\n"
     ]
    }
   ],
   "source": [
    "baseline(X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89c6da8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for OLS\n",
      "Training/In-Sample:  0.17101605821415816 \n",
      "Validation/Out-of-Sample:  0.1760599385357889\n"
     ]
    }
   ],
   "source": [
    "# create the model object\n",
    "def ols()\n",
    "    lm = LinearRegression(normalize=True)\n",
    "\n",
    "    # fit the model to our training data. We must specify the column in y_train, \n",
    "    # since we have converted it to a dataframe from a series! \n",
    "    lm.fit(X_train_scaled, y_train.logerror)\n",
    "\n",
    "    # predict train\n",
    "    y_train['logerror_pred_lm'] = lm.predict(X_train_scaled)\n",
    "\n",
    "    # evaluate: rmse\n",
    "    rmse_train = mean_squared_error(y_train.logerror, y_train.logerror_pred_lm)**(1/2)\n",
    "\n",
    "    # predict validate\n",
    "    y_validate['logerror_pred_lm'] = lm.predict(X_validate_scaled)\n",
    "\n",
    "    # evaluate: rmse\n",
    "    rmse_validate = mean_squared_error(y_validate.logerror, y_validate.logerror_pred_lm)**(1/2)\n",
    "\n",
    "    print(\"RMSE for OLS\\nTraining/In-Sample: \", rmse_train, \n",
    "          \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24a11b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# make the polynomial features to get a new set of features\n",
    "pf = PolynomialFeatures(degree=2)\n",
    "\n",
    "# fit and transform X_train_scaled\n",
    "X_train_degree2 = pf.fit_transform(X_train_scaled)\n",
    "\n",
    "# transform X_validate_scaled & X_test_scaled\n",
    "X_validate_degree2 = pf.transform(X_validate_scaled)\n",
    "X_test_degree2 = pf.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e129f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Poly 2\n",
      "Training/In-Sample:  0.17099215888763386 \n",
      "Validation/Out-of-Sample:  0.176063669620492\n"
     ]
    }
   ],
   "source": [
    "# create the model object\n",
    "lm2 = LinearRegression(normalize=True)\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "lm2.fit(X_train_degree2, y_train.logerror)\n",
    "\n",
    "# predict train\n",
    "y_train['logerror_pred_lm2'] = lm2.predict(X_train_degree2)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.logerror, y_train.logerror_pred_lm2)**(1/2)\n",
    "\n",
    "# predict validate\n",
    "y_validate['logerror_pred_lm2'] = lm2.predict(X_validate_degree2)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_validate = mean_squared_error(y_validate.logerror, y_validate.logerror_pred_lm2)**(1/2)\n",
    "\n",
    "print(\"RMSE for Poly 2\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fad30763",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.hist(y_validate.logerror, color='red', alpha=.5, label=\"Absolute Value Log Error\")\n",
    "plt.hist(y_validate.logerror_pred_lm, color='blue', alpha=.5, label=\"Validate\")\n",
    "plt.hist(y_test.logerror_pred_lm, color='green', alpha=.5, label='Test')\n",
    "plt.xlabel(\"Log Error\")\n",
    "plt.ylabel(\"Homes\")\n",
    "plt.title(\"Comparing the Distribution of Absolute Value Log Error to Predicted w/Validate and w/Test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04f1f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function for the LassoLars regression model object\n",
    "\n",
    "def lasso_lars(X_train_scaled, X_validate_scaled, X_test_scaled, y_train, y_validate, y_test):\n",
    "    lars = LassoLars(alpha=1.0)\n",
    "\n",
    "    # fit the model to our training data. We must specify the column in y_train, \n",
    "    # since we have converted it to a dataframe from a series! \n",
    "    lars.fit(X_train_scaled, y_train.logerror)\n",
    "\n",
    "    # predict train\n",
    "    y_train['logerror_pred_lars'] = lars.predict(X_train_scaled)\n",
    "\n",
    "    # evaluate: rmse\n",
    "    rmse_train = mean_squared_error(y_train.logerror, y_train.logerror_pred_lars)**(1/2)\n",
    "\n",
    "    # predict validate\n",
    "    y_validate['logerror_pred_lars'] = lars.predict(X_validate_scaled)\n",
    "\n",
    "    # evaluate: rmse\n",
    "    rmse_validate = mean_squared_error(y_validate.logerror, y_validate.logerror_pred_lars)**(1/2)\n",
    "\n",
    "    # predict test\n",
    "    #y_test['logerror_pred_lars'] = lars.predict(X_test_scaled)\n",
    "\n",
    "    # evaluate: rmse\n",
    "    #rmse_test = mean_squared_error(y_test.logerror, y_test.logerror_pred_lars)**(1/2)\n",
    "\n",
    "    print(\"RMSE for Lasso + Lars\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)\n",
    "      #\"\\nTesting/Out-of-Sample Performance: \", rmse_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e95b6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Lasso + Lars\n",
      "Training/In-Sample:  0.17111493570816058 \n",
      "Validation/Out-of-Sample:  0.1761900413468545\n"
     ]
    }
   ],
   "source": [
    "lasso_lars(X_train_scaled, X_validate_scaled, X_test_scaled, y_train, y_validate, y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af8f0a67",
   "metadata": {},
   "source": [
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.hist(y_validate.logerror, color='red', alpha=.5, label=\"Absolute Value Log Error\")\n",
    "plt.hist(y_validate.logerror_pred_lm, color='blue', alpha=.5, label=\"Validate\")\n",
    "plt.hist(y_test.logerror_pred_lm, color='green', alpha=.5, label='Test')\n",
    "plt.xlabel(\"Log Error\")\n",
    "plt.ylabel(\"Homes\")\n",
    "plt.title(\"Comparing the Distribution of Absolute Value Log Error to Predicted w/Validate and w/Test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc5a60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function for the Tweedie regression model object\n",
    "def tweedie(X_train_scaled, X_validate_scaled, X_test_scaled, y_train, y_validate, y_test):\n",
    "    glm = TweedieRegressor(power=0, alpha=1)\n",
    "\n",
    "    # fit the model to our training data. We must specify the column in y_train, \n",
    "    # since we have converted it to a dataframe from a series! \n",
    "    glm.fit(X_train_scaled, y_train.logerror)\n",
    "\n",
    "    # predict train\n",
    "    y_train['logerror_pred_glm'] = glm.predict(X_train_scaled)\n",
    "\n",
    "    # evaluate: rmse\n",
    "    rmse_train = mean_squared_error(y_train.logerror, y_train.logerror_pred_glm)**(1/2)\n",
    "\n",
    "    # predict validate\n",
    "    y_validate['logerror_pred_glm'] = glm.predict(X_validate_scaled)\n",
    "\n",
    "    # evaluate: rmse\n",
    "    rmse_validate = mean_squared_error(y_validate.logerror, y_validate.logerror_pred_glm)**(1/2)\n",
    "\n",
    "    # predict test\n",
    "    #y_test['logerror_pred_glm'] = glm.predict(X_test_scaled)\n",
    "    \n",
    "    # evaluate: rmse\n",
    "    #rmse_test = mean_squared_error(y_test.logerror, y_test.logerror_pred_glm)**(1/2)\n",
    "    \n",
    "    print(\"RMSE for Tweedie\\nTraining/In-Sample: \", rmse_train,\n",
    "          \"\\nValidation/Out-of-Sample: \", rmse_validate)\n",
    "          #\"\\nTesting/Out-of-Sample Performance: \", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "478117cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Tweedie\n",
      "Training/In-Sample:  0.1711068521696821 \n",
      "Validation/Out-of-Sample:  0.17618126011791158\n"
     ]
    }
   ],
   "source": [
    "tweedie(X_train_scaled, X_validate_scaled, X_test_scaled, y_train, y_validate, y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b8ae2c0",
   "metadata": {},
   "source": [
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.hist(y_validate.logerror, color='red', alpha=.5, label=\"Absolute Value Log Error\")\n",
    "plt.hist(y_validate.logerror_pred_lm, color='blue', alpha=.5, label=\"Validate\")\n",
    "plt.hist(y_test.logerror_pred_lm, color='green', alpha=.5, label='Test')\n",
    "plt.xlabel(\"Log Error\")\n",
    "plt.ylabel(\"Homes\")\n",
    "plt.title(\"Comparing the Distribution of Absolute Value Log Error to Predicted w/Validate and w/Test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63964d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7711b65c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
